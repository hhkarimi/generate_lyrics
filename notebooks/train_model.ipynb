{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## based on:\n",
    "## https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/code/ch16/ch16.ipynb\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## load embedding matrix and vocabulary\n",
    "path = './data/'\n",
    "vocab = np.load(path + 'vocab.npy')\n",
    "embedding_matrix = np.load(path + 'embedding_matrix.npy') # shape = [vocab_size, embedding_size]\n",
    "\n",
    "# create dictionary{word: int}\n",
    "token2int = {}\n",
    "for i, token in enumerate(vocab):\n",
    "    token2int[token] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eagles</td>\n",
       "      <td>After The Thrill Is Gone</td>\n",
       "      <td>[Verse]\\nSame dances in the same old shoes\\nSo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eagles</td>\n",
       "      <td>All She Wants To Do Is Dance</td>\n",
       "      <td>They're pickin' up the prisoners\\nAnd putting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eagles</td>\n",
       "      <td>Already Gone</td>\n",
       "      <td>[Verse 1]\\nWell, I heard some people talkin' j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eagles</td>\n",
       "      <td>Best of My Love</td>\n",
       "      <td>[Verse 1]\\nEvery night I'm lying in bed\\nHoldi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eagles</td>\n",
       "      <td>Bitter Creek</td>\n",
       "      <td>Once I was young and so unsure\\nI'd try any il...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   artist                          song  \\\n",
       "0  Eagles      After The Thrill Is Gone   \n",
       "1  Eagles  All She Wants To Do Is Dance   \n",
       "2  Eagles                  Already Gone   \n",
       "3  Eagles               Best of My Love   \n",
       "4  Eagles                  Bitter Creek   \n",
       "\n",
       "                                              lyrics  \n",
       "0  [Verse]\\nSame dances in the same old shoes\\nSo...  \n",
       "1  They're pickin' up the prisoners\\nAnd putting ...  \n",
       "2  [Verse 1]\\nWell, I heard some people talkin' j...  \n",
       "3  [Verse 1]\\nEvery night I'm lying in bed\\nHoldi...  \n",
       "4  Once I was young and so unsure\\nI'd try any il...  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load data\n",
    "with open('./data/lyrics_by_artist/Eagles.json') as f:\n",
    "    tmp = json.load(f)\n",
    "data = pd.DataFrame(columns=['artist', 'song', 'lyrics'])\n",
    "for i in range(5):\n",
    "    data = data.append({'artist': tmp['artists'][0]['artist'],\n",
    "                        'song': tmp['artists'][0]['songs'][i]['title'],\n",
    "                         'lyrics': tmp['artists'][0]['songs'][i]['lyrics']},\n",
    "                         ignore_index=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp['artists'][0]['songs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prep the data\n",
    "# convert words/tokens to integers from dictionary\n",
    "data['lyrics'] = data['lyrics'].apply(lambda text: word_tokenize(text.replace('\\n', '. ')))\n",
    "\n",
    "# reshape: batches and sequence_length, clip/pad based on mean/min/max song length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build LSTM model\n",
    "\n",
    "class LyricsGenerator(object):\n",
    "    def __init__(self, token2int, embedding_matrix, batch_size=64, seq_len=100, num_nodes=128,\n",
    "                 num_layers=1, learning_rate=0.001, keep_prob=0.5, grad_clip=5, \n",
    "                 sampling=False):\n",
    "        self.num_tokens = len(token2int)\n",
    "        self.token2int = token2int\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.num_lays = num_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.keep_prob = keep_prob\n",
    "        self.grad_clip = grad_clip\n",
    "        \n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(21)\n",
    "            \n",
    "            self.build(sampling=sampling)\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            \n",
    "    def build(self, sampling):\n",
    "        if sampling == True:\n",
    "            batch_size, seq_len = 1, 1\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "            seq_len = self.seq_len\n",
    "            \n",
    "        tf_x = tf.placeholder(tf.int32, shape=[batch_size, seq_len], name='tf_x')\n",
    "        tf_y = tf.placeholder(tf.int32, shape=[batch_size, seq_len], name='tf_y')\n",
    "        tf_keep_prob = tf.placeholder(tf.float32, name='tf_keep_prob')\n",
    "        \n",
    "        # load the embedding layer\n",
    "        embedding = tf.constant(self.embedding_matrix, name='embedding')\n",
    "        embed_x = tf.nn.embedding_lookup(embedding, tf_x, name='embedded_x')\n",
    "        \n",
    "#         # one-hot encoding\n",
    "#         x_onehot = tf.one_hot(tf_x, depth=self.num_tokens)\n",
    "#         y_onehot = tf.one_hot(tf_y, depth=self.num_tokens)\n",
    "        \n",
    "        # build the multi-layer LSTM cells\n",
    "        cells = tf.contrib.rnn.MultiRNNCell(\n",
    "            [tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(self.num_nodes), output_keep_prob=tf_keep_prob)\n",
    "            for _ in range(0, self.num_layers)])\n",
    "        \n",
    "        # set initial state\n",
    "        self.initial_state = cells.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        # run each sequence step through the RNN\n",
    "#         lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n",
    "#             cells, x_onehot, initial_state=self.initial_state)\n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n",
    "            cells, tf_x, initial_state=self.initial_state)\n",
    "        print('lstm_outputs:', lstm_outputs)\n",
    "        \n",
    "        seq_output_reshaped = tf.reshape(lstm_outputs, shape=[-1, self.num_nodes],\n",
    "                                         name='seq_output_reshaped')\n",
    "        \n",
    "        logits = tf.layers.dense(inputs=seq_output_reshaped, units=self.num_tokens,\n",
    "                                 activation=None, name='logits')\n",
    "        \n",
    "        proba = tf.nn.softmax(logits, name='probabilities')\n",
    "        print(proba)\n",
    "        \n",
    "#         y_reshaped = tf.reshape(y_onehot, shape=[-1, self.num_tokens], name='y_reshaped')\n",
    "        y_reshaped = tf.reshape(y, shape=[-1, self.num_tokens], name='y_reshaped')\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=y_reshaped), name='cost')\n",
    "        \n",
    "        # gradient clipping to avoid exploding gradients\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), self.grad_clip)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvards), name='train_op')\n",
    "        \n",
    "    def train(self, train_x, train_y, num_epochs, ckpt_dir='./model/'):\n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.mkdir(ckpt_dir)\n",
    "            \n",
    "        with tf.Session(graph=self.g) as session:\n",
    "            session.run(self.init_op)\n",
    "            \n",
    "            n_batches = int(train_x.shape[1]/self.seq_len) # check this arithmetic\n",
    "            iterations = n_batches * num_epochs\n",
    "            for epoch in range(0, num_epochs):\n",
    "                # train network\n",
    "                new_state = session.run(self.initial_state)\n",
    "                loss = 0\n",
    "                # minibatch operator\n",
    "                bgen = create_batch_generator(train_x, train_y, self.seq_len)\n",
    "                for b, (batch_x, batch_y) in enumerate(bgen, 1):\n",
    "                    iteration = epoch*n_batches + b\n",
    "                    \n",
    "                    feed = {'tf_x:0': batch_x, 'tf_y:0': batch_y, \n",
    "                            'tf_keep_prob:0': self.keep_prob, self.initial_state: new_state}\n",
    "                    batch_cost, _, new_state = session.run(\n",
    "                        ['cost:0', 'train_op', self.final_state], feed_dict=feed)\n",
    "                    \n",
    "                    if iteration % 10 == 0:\n",
    "                        print('Epoch {:d}/{:d} Iteration {:d} | Training loss: {:.4f}'.format(\n",
    "                            epoch+1, num_epochs, iteration, batch_cost))\n",
    "                        \n",
    "                ## save trained model\n",
    "                self.saver.save(session, os.path.join(\n",
    "                    ckpt_dir, 'lyrics_generator.ckpt'))\n",
    "                \n",
    "    def sample(self, output_length, ckpt_dir, starter_tokens=[\"The\", \"rain\"]):\n",
    "        with tf.Session(graph=self.g) as session:\n",
    "            self.saver.restore(session, tf.train.latest_checkpoint(ckpt_dir))\n",
    "            \n",
    "            # 1: run the model using starter tokens\n",
    "            new_state = sess.run(self.initial_state)\n",
    "            for token in starter_tokens:\n",
    "                x = np.zeros((1,1))\n",
    "                x[0, 0] = dictionary[token]\n",
    "                \n",
    "                feed = {'tf_x:0': x, 'tf_keep_prob:0': 1.0, self.initial_state: new_state}\n",
    "                proba, new_state = session.run(\n",
    "                    ['probabilities:0', self.final_state], feed_dict=feed)\n",
    "                \n",
    "            token_id = self.get_top_token(proba, len(vocab))\n",
    "            observed_seq.append(vocab[token_id])\n",
    "                \n",
    "            # 2: run model using updated observed_seq\n",
    "            for i in range(0, output_length):\n",
    "                x[0, 0] = token_id\n",
    "                feed = {'tf_x:0': x, 'tf_keep_prob:0': 1.0,\n",
    "                        self.initial_state: new_state}\n",
    "                proba, new_state = session.run(\n",
    "                    ['probabilities:0', self.final_state], feed_dict=feed)\n",
    "                \n",
    "                token_id = self.get_top_token(proba, len(vocab))\n",
    "                observed_seq.append(vocab[token_id])\n",
    "                \n",
    "            return ''.join(observed_seq)\n",
    "        \n",
    "    def get_top_token(self, probas, token_size, top_n=5):\n",
    "        p = np.squeeze(probas)\n",
    "        p[np.argsort(p)[:-top_n]] = 0.0\n",
    "        p = p / np.sum(p)\n",
    "        token_id = np.random.choice(char_size, 1, p=p)[0]\n",
    "        return token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'serialized_options'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-3f89aee4495a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLyricsGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken2int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-114-0caf87464dc9>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, token2int, embedding_matrix, batch_size, seq_len, num_nodes, num_layers, learning_rate, keep_prob, grad_clip, sampling)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-114-0caf87464dc9>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, sampling)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# build the multi-layer LSTM cells\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         cells = tf.contrib.rnn.MultiRNNCell(\n\u001b[0m\u001b[1;32m     46\u001b[0m             [tf.contrib.rnn.DropoutWrapper(\n\u001b[1;32m     47\u001b[0m                 tf.contrib.rnn.BasicLSTMCell(self.num_nodes), output_keep_prob=tf_keep_prob)\n",
      "\u001b[0;32m/wayfair/home/hkarimi/venvs/py36_kernel_dev/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/wayfair/home/hkarimi/venvs/py36_kernel_dev/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Import the target module and insert it into the parent's namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_module_globals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/wayfair/home/hkarimi/venvs/py36_kernel_dev/lib64/python3.6/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/wayfair/home/hkarimi/venvs/py36_kernel_dev/lib64/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m/wayfair/home/hkarimi/venvs/py36_kernel_dev/lib64/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/wayfair/home/hkarimi/venvs/py36_kernel_dev/lib64/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/wayfair/home/hkarimi/venvs/py36_kernel_dev/lib64/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/wayfair/home/hkarimi/venvs/py36_kernel_dev/lib64/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/wayfair/home/hkarimi/venvs/py36_kernel_dev/lib64/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m/wayfair/home/hkarimi/venvs/py36_kernel_dev/lib/python3.6/site-packages/tensorflow/contrib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstat_summarizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstateless\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_forest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/wayfair/home/hkarimi/venvs/py36_kernel_dev/lib/python3.6/site-packages/tensorflow/contrib/tensor_forest/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# pylint: disable=unused-import,wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: enable=unused-import,wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/wayfair/home/hkarimi/venvs/py36_kernel_dev/lib/python3.6/site-packages/tensorflow/contrib/tensor_forest/client/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meval_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom_forest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: enable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/wayfair/home/hkarimi/venvs/py36_kernel_dev/lib/python3.6/site-packages/tensorflow/contrib/tensor_forest/client/random_forest.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meval_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_forest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/wayfair/home/hkarimi/venvs/py36_kernel_dev/lib/python3.6/site-packages/tensorflow/contrib/tensor_forest/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_forest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/wayfair/home/hkarimi/venvs/py36_kernel_dev/lib/python3.6/site-packages/tensorflow/contrib/tensor_forest/python/tensor_forest.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtext_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_trees\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeneric_tree_model_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_tree_proto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvariables\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mframework_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_forest_params_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_params_proto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/wayfair/home/hkarimi/venvs/py36_kernel_dev/lib/python3.6/site-packages/tensorflow/contrib/decision_trees/proto/generic_tree_model_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0many_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgoogle_dot_protobuf_dot_any__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwrappers_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgoogle_dot_protobuf_dot_wrappers__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/wayfair/home/hkarimi/venvs/py36_kernel_dev/lib/python3.6/site-packages/google/protobuf/wrappers_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0msyntax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'proto3'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mserialized_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\023com.google.protobufB\\rWrappersProtoP\\001Z*github.com/golang/protobuf/ptypes/wrappers\\370\\001\\001\\242\\002\\003GPB\\252\\002\\036Google.Protobuf.WellKnownTypes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mserialized_pb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\x1egoogle/protobuf/wrappers.proto\\x12\\x0fgoogle.protobuf\\\"\\x1c\\n\\x0b\\x44oubleValue\\x12\\r\\n\\x05value\\x18\\x01 \\x01(\\x01\\\"\\x1b\\n\\nFloatValue\\x12\\r\\n\\x05value\\x18\\x01 \\x01(\\x02\\\"\\x1b\\n\\nInt64Value\\x12\\r\\n\\x05value\\x18\\x01 \\x01(\\x03\\\"\\x1c\\n\\x0bUInt64Value\\x12\\r\\n\\x05value\\x18\\x01 \\x01(\\x04\\\"\\x1b\\n\\nInt32Value\\x12\\r\\n\\x05value\\x18\\x01 \\x01(\\x05\\\"\\x1c\\n\\x0bUInt32Value\\x12\\r\\n\\x05value\\x18\\x01 \\x01(\\r\\\"\\x1a\\n\\tBoolValue\\x12\\r\\n\\x05value\\x18\\x01 \\x01(\\x08\\\"\\x1c\\n\\x0bStringValue\\x12\\r\\n\\x05value\\x18\\x01 \\x01(\\t\\\"\\x1b\\n\\nBytesValue\\x12\\r\\n\\x05value\\x18\\x01 \\x01(\\x0c\\x42|\\n\\x13\\x63om.google.protobufB\\rWrappersProtoP\\x01Z*github.com/golang/protobuf/ptypes/wrappers\\xf8\\x01\\x01\\xa2\\x02\\x03GPB\\xaa\\x02\\x1eGoogle.Protobuf.WellKnownTypesb\\x06proto3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'serialized_options'"
     ]
    }
   ],
   "source": [
    "lstm = LyricsGenerator(token2int, embedding_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 dev",
   "language": "python",
   "name": "py36_kernel_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
