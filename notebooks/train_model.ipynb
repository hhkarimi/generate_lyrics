{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist = 'Eric Clapton'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "## based on:\n",
    "## https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/code/ch16/ch16.ipynb\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load embedding matrix and vocabulary\n",
    "path = './data/'\n",
    "int2token = np.load(path + 'vocab.npy')\n",
    "embedding_matrix = np.load(path + 'embedding_matrix.npy') # shape = [vocab_size, embedding_size]\n",
    "\n",
    "# create dictionary{word: int}\n",
    "token2int = {token: i for i, token in enumerate(int2token)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eric Clapton</td>\n",
       "      <td>32-20</td>\n",
       "      <td>\\nI walked all night long\\nWith my 32-20 in my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eric Clapton</td>\n",
       "      <td>32-20 Blues</td>\n",
       "      <td>\\nIf I send for my baby, man, and she doesn't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eric Clapton</td>\n",
       "      <td>44</td>\n",
       "      <td>I wore my .44 so long, I made my shoulder sore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eric Clapton</td>\n",
       "      <td>A Certain Girl</td>\n",
       "      <td>1\\nThere is a certain girl I have been in lov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eric Clapton</td>\n",
       "      <td>After Midnight</td>\n",
       "      <td>\\nAfter midnight, we're going to let it all ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         artist            song  \\\n",
       "0  Eric Clapton           32-20   \n",
       "1  Eric Clapton     32-20 Blues   \n",
       "2  Eric Clapton              44   \n",
       "3  Eric Clapton  A Certain Girl   \n",
       "4  Eric Clapton  After Midnight   \n",
       "\n",
       "                                              lyrics  \n",
       "0  \\nI walked all night long\\nWith my 32-20 in my...  \n",
       "1  \\nIf I send for my baby, man, and she doesn't ...  \n",
       "2  I wore my .44 so long, I made my shoulder sore...  \n",
       "3   1\\nThere is a certain girl I have been in lov...  \n",
       "4  \\nAfter midnight, we're going to let it all ha...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load lyrics\n",
    "with open('./data/lyrics_by_artist/{}.json'.format(artist).replace(' ','')) as f:\n",
    "    tmp = json.load(f)\n",
    "data = pd.DataFrame(columns=['artist', 'song', 'lyrics'])\n",
    "for i in range(0, len(tmp['artists'][0]['songs'])):\n",
    "    data = data.append({'artist': tmp['artists'][0]['artist'],\n",
    "                        'song': tmp['artists'][0]['songs'][i]['title'],\n",
    "                         'lyrics': tmp['artists'][0]['songs'][i]['lyrics']},\n",
    "                         ignore_index=True)\n",
    "    \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 175 ms, sys: 8.21 ms, total: 183 ms\n",
      "Wall time: 181 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# tokenize lyrics\n",
    "for i, lyrics in enumerate(data['lyrics']):\n",
    "    text = lyrics\n",
    "    for break_word in ['1\\n', '2\\n', '3\\n', '4\\n', '5\\n', '1 \\n', '2 \\n', '3 \\n', '4 \\n', '5 \\n']:\n",
    "        text = text.replace(break_word, '')\n",
    "    text = text.replace('\\n', ' line_break ')\n",
    "    text = text.split()\n",
    "    text = [word if word not in punctuation else ' '+word+' ' for word in text]\n",
    "    text = [word if word != 'line_break' else '\\n' for word in text]\n",
    "\n",
    "    data.loc[i, 'lyrics'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter()\n",
    "for i, song in enumerate(data['lyrics']):\n",
    "    counts.update(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\n', 11966), ('I', 2718), ('you', 1956), ('the', 1908), ('to', 1804)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23 ms, sys: 1.06 ms, total: 24.1 ms\n",
      "Wall time: 23.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# convert words/tokens to integers from dictionary\n",
    "def tokens_to_int(tokens):\n",
    "    mapped_tokens = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            mapped_tokens.append(token2int[token])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return mapped_tokens\n",
    "data['lyrics'] = data['lyrics'].apply(tokens_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## contatenate words together\n",
    "## https://github.com/hunkim/word-rnn-tensorflow/tree/master/data/tinyshakespeare/input.txt\n",
    "tmp = []\n",
    "count = 0\n",
    "for lyrics in data['lyrics']:\n",
    "    count = count + len(lyrics)\n",
    "    tmp.extend(lyrics)\n",
    "data = np.array(tmp)\n",
    "assert len(data) == count\n",
    "del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: [0 1 2 3 4 5 0 6 7 8 7]\n",
      "y_train: [1 2 3 4 5 0 6 7 8 7]\n",
      "\n",
      "Stats:\n",
      "Percent of data lost: 1.76%\n",
      "mini_seq_len = 128\n",
      "\n",
      "Beginning of Input:\n",
      " \n",
      " I walked all night long \n",
      " With my in my hand \n",
      " I walked all night long \n",
      " With my in my hand \n",
      " Looking for my woman \n",
      " Well, I found her with another man \n",
      " \n",
      " When I found that woman \n",
      " They were walking hand in\n"
     ]
    }
   ],
   "source": [
    "## split into batch_size batches, each batch with subsequences of size mini_seq_len\n",
    "## so number of subsequences per batch is len(sequence)/(batch_size * mini_seq_len)\n",
    "\n",
    "def reshape_data(sequence, batch_size, mini_seq_len):\n",
    "    tot_batch_length = batch_size * mini_seq_len\n",
    "    num_seq_per_batch = int(len(sequence) / tot_batch_length)\n",
    "    if num_seq_per_batch*tot_batch_length + 1 > len(sequence):\n",
    "        num_seq_per_batch = num_seq_per_batch - 1\n",
    "    ## Truncate the sequence at the end to get rid of \n",
    "    ## remaining charcaters that do not make a full batch\n",
    "    x = sequence[0 : num_seq_per_batch*tot_batch_length]\n",
    "    y = sequence[1 : num_seq_per_batch*tot_batch_length + 1]\n",
    "    ## Split x & y into a list batches of sequences: \n",
    "    x_batch_splits = np.split(x, batch_size)\n",
    "    y_batch_splits = np.split(y, batch_size)\n",
    "    ## Stack the batches together\n",
    "    ## batch_size x mini_batch_length\n",
    "    x = np.stack(x_batch_splits)\n",
    "    y = np.stack(y_batch_splits)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "\n",
    "# set data shape parameters\n",
    "batch_size = 16\n",
    "mini_seq_len = 128\n",
    "\n",
    "# reshape and check\n",
    "x_train, y_train = reshape_data(data, batch_size=batch_size, mini_seq_len=mini_seq_len)\n",
    "print('x_train:', x_train[0, :11])\n",
    "print('y_train:', y_train[0, :10])\n",
    "\n",
    "# interesting stats\n",
    "mini_seq_per_batch = int( len(data) / (batch_size*mini_seq_len) )\n",
    "total_tokens_kept = mini_seq_per_batch * mini_seq_len * batch_size\n",
    "num_seq_per_batch = mini_seq_per_batch * mini_seq_len\n",
    "print('\\nStats:')\n",
    "print('Percent of data lost: {:.2f}%'.format(100*( len(data)-len(x_train.flatten()) )  / len(data)))\n",
    "print('mini_seq_len = {}'.format(mini_seq_len))\n",
    "\n",
    "# print data pull sample\n",
    "sample = ' '.join(int2token[i] for i in x_train[0, :50])\n",
    "# print('\\nBeginning of Input:\\n', sample.replace(\" '\", \"'\"))\n",
    "print('\\nBeginning of Input:\\n', sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## set target to predict the next word in the sequence, so y is x offset by 1 position\n",
    "# train_percentage = 0.8\n",
    "# train_length = int(train_percentage*len(data))\n",
    "\n",
    "# # split data, offset target by 1 position, drop the last word for inputs\n",
    "# X_train = sequences[:train_length, :]\n",
    "# y_train = X_train[:, 1:]\n",
    "# X_train = X_train[:, :-1]\n",
    "\n",
    "# X_test = sequences[train_length:, :]\n",
    "# y_test = np.zeros(X_test.shape, dtype=int)\n",
    "# y_test[:, :-1] = X_test[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('X_train:\\n', X_train[:3,:])\n",
    "# print('y_train:\\n', y_train[:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build LSTM model\n",
    "\n",
    "class LyricsGeneratorNN(object):\n",
    "    def __init__(self, token2int, int2token, embedding_matrix, mini_seq_len=100, batch_size=16,\n",
    "                 num_nodes=128, num_layers=1, learning_rate=0.001, keep_prob=0.5, grad_clip=5, \n",
    "                 sampling=False):\n",
    "        self.num_tokens = len(token2int)\n",
    "        self.token2int = token2int\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.mini_seq_len = mini_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_layers = num_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.keep_prob = keep_prob\n",
    "        self.grad_clip = grad_clip\n",
    "        \n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(21)\n",
    "            \n",
    "            self.build(sampling=sampling)\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            \n",
    "            \n",
    "    def build(self, sampling):\n",
    "        if sampling == True:\n",
    "            batch_size, mini_seq_len = 1, 1\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "            mini_seq_len = self.mini_seq_len\n",
    "            \n",
    "        tf_x = tf.placeholder(tf.int32, shape=[batch_size, mini_seq_len], name='tf_x')\n",
    "        tf_y = tf.placeholder(tf.int32, shape=[batch_size, mini_seq_len], name='tf_y')\n",
    "        tf_keep_prob = tf.placeholder(tf.float32, name='tf_keep_prob')\n",
    "        \n",
    "        # load the embedding layer\n",
    "        embedding = tf.constant(self.embedding_matrix, name='embedding')\n",
    "        embed_x = tf.nn.embedding_lookup(embedding, tf_x, name='embedded_x')\n",
    "        \n",
    "        # one-hot encoding\n",
    "        x_onehot = tf.one_hot(tf_x, depth=self.num_tokens)\n",
    "        y_onehot = tf.one_hot(tf_y, depth=self.num_tokens)\n",
    "        \n",
    "        # build the multi-layer LSTM cells\n",
    "        cells = tf.contrib.rnn.MultiRNNCell(\n",
    "            [tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(self.num_nodes), output_keep_prob=tf_keep_prob)\n",
    "            for _ in range(0, self.num_layers)])\n",
    "        \n",
    "        # set initial state\n",
    "        self.initial_state = cells.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        # run each sequence step through the RNN\n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n",
    "            cells, embed_x, initial_state=self.initial_state)\n",
    "        print('lstm_outputs:', lstm_outputs)\n",
    "        \n",
    "        seq_output_reshaped = tf.reshape(lstm_outputs, shape=[-1, self.num_nodes],\n",
    "                                         name='seq_output_reshaped')\n",
    "        \n",
    "        logits = tf.layers.dense(inputs=seq_output_reshaped, units=self.num_tokens,\n",
    "                                 activation=None, name='logits')\n",
    "        \n",
    "        proba = tf.nn.softmax(logits, name='probabilities')\n",
    "        print(proba)\n",
    "        \n",
    "        y_reshaped = tf.reshape(y_onehot, shape=[-1, self.num_tokens], name='y_reshaped')\n",
    "        \n",
    "        try:\n",
    "            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                logits=logits, labels=y_reshaped), name='cost')\n",
    "        except AttributeError:\n",
    "            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=y_reshaped), name='cost')\n",
    "        \n",
    "        # gradient clipping to avoid exploding gradients\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), self.grad_clip)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars), name='train_op')\n",
    "        \n",
    "        \n",
    "    def train(self, x_train, y_train, num_epochs, ckpt_dir='./model/'):\n",
    "        # set up checkpoint for saving\n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.mkdir(ckpt_dir)\n",
    "            \n",
    "        with tf.Session(graph=self.g) as session:\n",
    "            session.run(self.init_op)\n",
    "            \n",
    "            mini_seq_per_batch = int(x_train.shape[1]/self.mini_seq_len)\n",
    "            iterations = mini_seq_per_batch * num_epochs\n",
    "            for epoch in range(0, num_epochs):\n",
    "                \n",
    "                # train network\n",
    "                new_state = session.run(self.initial_state)\n",
    "                loss = 0\n",
    "                \n",
    "                # minibatch operator\n",
    "                generated_batch = self.create_batch_generator(x_train, y_train, self.mini_seq_len)\n",
    "                for b, (x_batch, y_batch) in enumerate(generated_batch, 1):\n",
    "                    iteration = epoch*mini_seq_per_batch + b\n",
    "                    \n",
    "                    feed = {'tf_x:0': x_batch, 'tf_y:0': y_batch, \n",
    "                            'tf_keep_prob:0': self.keep_prob, self.initial_state: new_state}\n",
    "                    batch_cost, _, new_state = session.run(\n",
    "                        ['cost:0', 'train_op', self.final_state], feed_dict=feed)\n",
    "                    \n",
    "                    if iteration % 10 == 0:\n",
    "                        print('Epoch {:d}/{:d} Iteration {:d} | Training loss: {:.4f}'.format(\n",
    "                            epoch+1, num_epochs, iteration, batch_cost))\n",
    "                        \n",
    "                ## save trained model\n",
    "                self.saver.save(session, os.path.join(\n",
    "                    ckpt_dir, 'lyrics_generator.ckpt'))\n",
    "\n",
    "\n",
    "    def create_batch_generator(self, x, y, mini_seq_len):\n",
    "        batch_size, tokens_per_batch = x.shape\n",
    "        mini_seq_per_batch = int(tokens_per_batch/mini_seq_len)\n",
    "        for b in range(0, mini_seq_per_batch):\n",
    "            yield(x[:, b*mini_seq_len:(b+1)*mini_seq_len],\n",
    "                  y[:, b*mini_seq_len:(b+1)*mini_seq_len])\n",
    "    \n",
    "    \n",
    "    def sample(self, output_length, ckpt_dir, starter_tokens=[\"The\", \"rain\"], beam_search=False):\n",
    "        observed_seq = [token for token in starter_tokens]\n",
    "        with tf.Session(graph=self.g) as session:\n",
    "            self.saver.restore(session, tf.train.latest_checkpoint(ckpt_dir))\n",
    "            \n",
    "            # TODO: add beam_search\n",
    "            if beam_search:\n",
    "                print('TODO: come back again')\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            # 1: run the model using starter tokens\n",
    "            new_state = session.run(self.initial_state)\n",
    "            for token in starter_tokens:\n",
    "                x = np.zeros((1,1))\n",
    "                x[0, 0] = token2int[token]\n",
    "                \n",
    "                feed = {'tf_x:0': x, 'tf_keep_prob:0': 1.0, self.initial_state: new_state}\n",
    "                proba, new_state = session.run(\n",
    "                    ['probabilities:0', self.final_state], feed_dict=feed)\n",
    "                \n",
    "            token_id = self.get_top_token(proba, len(int2token))\n",
    "            observed_seq.append(int2token[token_id])\n",
    "                \n",
    "            # 2: run model using updated observed_seq\n",
    "            for i in range(0, output_length):\n",
    "                x[0, 0] = token_id\n",
    "                feed = {'tf_x:0': x, 'tf_keep_prob:0': 1.0,\n",
    "                        self.initial_state: new_state}\n",
    "                proba, new_state = session.run(\n",
    "                    ['probabilities:0', self.final_state], feed_dict=feed)\n",
    "                \n",
    "                token_id = self.get_top_token(proba, len(int2token))\n",
    "                observed_seq.append(int2token[token_id])\n",
    "                \n",
    "            return ' '.join(observed_seq)\n",
    "        \n",
    "        \n",
    "    def get_top_token(self, probas, token_size, top_n=5):\n",
    "        p = np.squeeze(probas)\n",
    "        p[np.argsort(p)[:-top_n]] = 0.0\n",
    "        p = p / np.sum(p)\n",
    "        token_id = np.random.choice(token_size, 1, p=p)[0]\n",
    "        return token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_outputs: Tensor(\"rnn/transpose_1:0\", shape=(16, 128, 256), dtype=float32)\n",
      "Tensor(\"probabilities:0\", shape=(2048, 3046), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## set data shape parameters\n",
    "# batch_size = 16 (set earlier)\n",
    "# mini_seq_len = 256 (set earlier)\n",
    "\n",
    "# reshape and check\n",
    "x_train, y_train = reshape_data(data, batch_size=batch_size, mini_seq_len=mini_seq_len)\n",
    "\n",
    "mini_seq_per_batch = int( len(data) / (batch_size*mini_seq_len) )\n",
    "total_tokens_kept = mini_seq_per_batch * mini_seq_len * batch_size\n",
    "num_seq_per_batch = mini_seq_per_batch * mini_seq_len\n",
    "\n",
    "# lstm parameters\n",
    "num_nodes = 256\n",
    "num_layers = 4\n",
    "lstm = LyricsGeneratorNN(token2int, int2token, embedding_matrix, mini_seq_len=mini_seq_len,\n",
    "                         batch_size=batch_size, num_nodes=num_nodes, num_layers=num_layers, sampling=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 Iteration 10 | Training loss: 6.1692\n",
      "Epoch 1/100 Iteration 20 | Training loss: 5.7536\n",
      "Epoch 1/100 Iteration 30 | Training loss: 5.7381\n",
      "Epoch 2/100 Iteration 40 | Training loss: 5.7352\n",
      "Epoch 2/100 Iteration 50 | Training loss: 5.6510\n",
      "Epoch 2/100 Iteration 60 | Training loss: 5.7592\n",
      "Epoch 2/100 Iteration 70 | Training loss: 5.6323\n",
      "Epoch 3/100 Iteration 80 | Training loss: 5.7553\n",
      "Epoch 3/100 Iteration 90 | Training loss: 5.5995\n",
      "Epoch 3/100 Iteration 100 | Training loss: 5.5892\n",
      "Epoch 4/100 Iteration 110 | Training loss: 5.6560\n",
      "Epoch 4/100 Iteration 120 | Training loss: 5.6536\n",
      "Epoch 4/100 Iteration 130 | Training loss: 5.7106\n",
      "Epoch 4/100 Iteration 140 | Training loss: 5.5836\n",
      "Epoch 5/100 Iteration 150 | Training loss: 5.7182\n",
      "Epoch 5/100 Iteration 160 | Training loss: 5.5746\n",
      "Epoch 5/100 Iteration 170 | Training loss: 5.5703\n",
      "Epoch 6/100 Iteration 180 | Training loss: 5.6299\n",
      "Epoch 6/100 Iteration 190 | Training loss: 5.6198\n",
      "Epoch 6/100 Iteration 200 | Training loss: 5.6901\n",
      "Epoch 6/100 Iteration 210 | Training loss: 5.5658\n",
      "Epoch 7/100 Iteration 220 | Training loss: 5.7002\n",
      "Epoch 7/100 Iteration 230 | Training loss: 5.5645\n",
      "Epoch 7/100 Iteration 240 | Training loss: 5.5549\n",
      "Epoch 8/100 Iteration 250 | Training loss: 5.6132\n",
      "Epoch 8/100 Iteration 260 | Training loss: 5.6062\n",
      "Epoch 8/100 Iteration 270 | Training loss: 5.6854\n",
      "Epoch 8/100 Iteration 280 | Training loss: 5.5431\n",
      "Epoch 9/100 Iteration 290 | Training loss: 5.6759\n",
      "Epoch 9/100 Iteration 300 | Training loss: 5.5574\n",
      "Epoch 9/100 Iteration 310 | Training loss: 5.5407\n",
      "Epoch 10/100 Iteration 320 | Training loss: 5.6021\n",
      "Epoch 10/100 Iteration 330 | Training loss: 5.5939\n",
      "Epoch 10/100 Iteration 340 | Training loss: 5.6635\n",
      "Epoch 10/100 Iteration 350 | Training loss: 5.5348\n",
      "Epoch 11/100 Iteration 360 | Training loss: 5.6479\n",
      "Epoch 11/100 Iteration 370 | Training loss: 5.4771\n",
      "Epoch 11/100 Iteration 380 | Training loss: 5.4671\n",
      "Epoch 12/100 Iteration 390 | Training loss: 5.5294\n",
      "Epoch 12/100 Iteration 400 | Training loss: 5.4983\n",
      "Epoch 12/100 Iteration 410 | Training loss: 5.5725\n",
      "Epoch 12/100 Iteration 420 | Training loss: 5.4404\n",
      "Epoch 13/100 Iteration 430 | Training loss: 5.5385\n",
      "Epoch 13/100 Iteration 440 | Training loss: 5.3264\n",
      "Epoch 13/100 Iteration 450 | Training loss: 5.3357\n",
      "Epoch 14/100 Iteration 460 | Training loss: 5.3404\n",
      "Epoch 14/100 Iteration 470 | Training loss: 5.2981\n",
      "Epoch 14/100 Iteration 480 | Training loss: 5.3966\n",
      "Epoch 14/100 Iteration 490 | Training loss: 5.2615\n",
      "Epoch 15/100 Iteration 500 | Training loss: 5.3146\n",
      "Epoch 15/100 Iteration 510 | Training loss: 5.1228\n",
      "Epoch 15/100 Iteration 520 | Training loss: 5.1386\n",
      "Epoch 16/100 Iteration 530 | Training loss: 5.1635\n",
      "Epoch 16/100 Iteration 540 | Training loss: 5.1222\n",
      "Epoch 16/100 Iteration 550 | Training loss: 5.1985\n",
      "Epoch 16/100 Iteration 560 | Training loss: 5.0860\n",
      "Epoch 17/100 Iteration 570 | Training loss: 5.1746\n",
      "Epoch 17/100 Iteration 580 | Training loss: 4.9356\n",
      "Epoch 17/100 Iteration 590 | Training loss: 4.9613\n",
      "Epoch 18/100 Iteration 600 | Training loss: 4.9891\n",
      "Epoch 18/100 Iteration 610 | Training loss: 4.9560\n",
      "Epoch 18/100 Iteration 620 | Training loss: 5.0704\n",
      "Epoch 18/100 Iteration 630 | Training loss: 4.9574\n",
      "Epoch 19/100 Iteration 640 | Training loss: 5.0591\n",
      "Epoch 19/100 Iteration 650 | Training loss: 4.8259\n",
      "Epoch 19/100 Iteration 660 | Training loss: 4.8361\n",
      "Epoch 20/100 Iteration 670 | Training loss: 4.8813\n",
      "Epoch 20/100 Iteration 680 | Training loss: 4.8494\n",
      "Epoch 20/100 Iteration 690 | Training loss: 4.9653\n",
      "Epoch 20/100 Iteration 700 | Training loss: 4.8509\n",
      "Epoch 21/100 Iteration 710 | Training loss: 4.9463\n",
      "Epoch 21/100 Iteration 720 | Training loss: 4.7070\n",
      "Epoch 21/100 Iteration 730 | Training loss: 4.7133\n",
      "Epoch 22/100 Iteration 740 | Training loss: 4.7705\n",
      "Epoch 22/100 Iteration 750 | Training loss: 4.7333\n",
      "Epoch 22/100 Iteration 760 | Training loss: 4.8940\n",
      "Epoch 22/100 Iteration 770 | Training loss: 4.7265\n",
      "Epoch 23/100 Iteration 780 | Training loss: 4.8187\n",
      "Epoch 23/100 Iteration 790 | Training loss: 4.6040\n",
      "Epoch 23/100 Iteration 800 | Training loss: 4.6011\n",
      "Epoch 24/100 Iteration 810 | Training loss: 4.6418\n",
      "Epoch 24/100 Iteration 820 | Training loss: 4.6264\n",
      "Epoch 24/100 Iteration 830 | Training loss: 4.7883\n",
      "Epoch 24/100 Iteration 840 | Training loss: 4.6275\n",
      "Epoch 25/100 Iteration 850 | Training loss: 4.7321\n",
      "Epoch 25/100 Iteration 860 | Training loss: 4.4929\n",
      "Epoch 25/100 Iteration 870 | Training loss: 4.5051\n",
      "Epoch 26/100 Iteration 880 | Training loss: 4.5559\n",
      "Epoch 26/100 Iteration 890 | Training loss: 4.5272\n",
      "Epoch 26/100 Iteration 900 | Training loss: 4.7165\n",
      "Epoch 26/100 Iteration 910 | Training loss: 4.5562\n",
      "Epoch 27/100 Iteration 920 | Training loss: 4.6743\n",
      "Epoch 27/100 Iteration 930 | Training loss: 4.4260\n",
      "Epoch 27/100 Iteration 940 | Training loss: 4.4422\n",
      "Epoch 28/100 Iteration 950 | Training loss: 4.4879\n",
      "Epoch 28/100 Iteration 960 | Training loss: 4.4794\n",
      "Epoch 28/100 Iteration 970 | Training loss: 4.6538\n",
      "Epoch 28/100 Iteration 980 | Training loss: 4.5053\n",
      "Epoch 29/100 Iteration 990 | Training loss: 4.5786\n",
      "Epoch 29/100 Iteration 1000 | Training loss: 4.3521\n",
      "Epoch 29/100 Iteration 1010 | Training loss: 4.3768\n",
      "Epoch 30/100 Iteration 1020 | Training loss: 4.4302\n",
      "Epoch 30/100 Iteration 1030 | Training loss: 4.4381\n",
      "Epoch 30/100 Iteration 1040 | Training loss: 4.5762\n",
      "Epoch 30/100 Iteration 1050 | Training loss: 4.4225\n",
      "Epoch 31/100 Iteration 1060 | Training loss: 4.5118\n",
      "Epoch 31/100 Iteration 1070 | Training loss: 4.3157\n",
      "Epoch 31/100 Iteration 1080 | Training loss: 4.3151\n",
      "Epoch 32/100 Iteration 1090 | Training loss: 4.3446\n",
      "Epoch 32/100 Iteration 1100 | Training loss: 4.3541\n",
      "Epoch 32/100 Iteration 1110 | Training loss: 4.5258\n",
      "Epoch 32/100 Iteration 1120 | Training loss: 4.3591\n",
      "Epoch 33/100 Iteration 1130 | Training loss: 4.4497\n",
      "Epoch 33/100 Iteration 1140 | Training loss: 4.2558\n",
      "Epoch 33/100 Iteration 1150 | Training loss: 4.2603\n",
      "Epoch 34/100 Iteration 1160 | Training loss: 4.2666\n",
      "Epoch 34/100 Iteration 1170 | Training loss: 4.2892\n",
      "Epoch 34/100 Iteration 1180 | Training loss: 4.4754\n",
      "Epoch 34/100 Iteration 1190 | Training loss: 4.3023\n",
      "Epoch 35/100 Iteration 1200 | Training loss: 4.4224\n",
      "Epoch 35/100 Iteration 1210 | Training loss: 4.1900\n",
      "Epoch 35/100 Iteration 1220 | Training loss: 4.2142\n",
      "Epoch 36/100 Iteration 1230 | Training loss: 4.2202\n",
      "Epoch 36/100 Iteration 1240 | Training loss: 4.2684\n",
      "Epoch 36/100 Iteration 1250 | Training loss: 4.4125\n",
      "Epoch 36/100 Iteration 1260 | Training loss: 4.3029\n",
      "Epoch 37/100 Iteration 1270 | Training loss: 4.3431\n",
      "Epoch 37/100 Iteration 1280 | Training loss: 4.1344\n",
      "Epoch 37/100 Iteration 1290 | Training loss: 4.1604\n",
      "Epoch 38/100 Iteration 1300 | Training loss: 4.2054\n",
      "Epoch 38/100 Iteration 1310 | Training loss: 4.2242\n",
      "Epoch 38/100 Iteration 1320 | Training loss: 4.3775\n",
      "Epoch 38/100 Iteration 1330 | Training loss: 4.2411\n",
      "Epoch 39/100 Iteration 1340 | Training loss: 4.2990\n",
      "Epoch 39/100 Iteration 1350 | Training loss: 4.1029\n",
      "Epoch 39/100 Iteration 1360 | Training loss: 4.1302\n",
      "Epoch 40/100 Iteration 1370 | Training loss: 4.1491\n",
      "Epoch 40/100 Iteration 1380 | Training loss: 4.1773\n",
      "Epoch 40/100 Iteration 1390 | Training loss: 4.3364\n",
      "Epoch 40/100 Iteration 1400 | Training loss: 4.2060\n",
      "Epoch 41/100 Iteration 1410 | Training loss: 4.2655\n",
      "Epoch 41/100 Iteration 1420 | Training loss: 4.0784\n",
      "Epoch 41/100 Iteration 1430 | Training loss: 4.0761\n",
      "Epoch 42/100 Iteration 1440 | Training loss: 4.1191\n",
      "Epoch 42/100 Iteration 1450 | Training loss: 4.1247\n",
      "Epoch 42/100 Iteration 1460 | Training loss: 4.3219\n",
      "Epoch 42/100 Iteration 1470 | Training loss: 4.1793\n",
      "Epoch 43/100 Iteration 1480 | Training loss: 4.2229\n",
      "Epoch 43/100 Iteration 1490 | Training loss: 4.0339\n",
      "Epoch 43/100 Iteration 1500 | Training loss: 4.0445\n",
      "Epoch 44/100 Iteration 1510 | Training loss: 4.0614\n",
      "Epoch 44/100 Iteration 1520 | Training loss: 4.0929\n",
      "Epoch 44/100 Iteration 1530 | Training loss: 4.2833\n",
      "Epoch 44/100 Iteration 1540 | Training loss: 4.1342\n",
      "Epoch 45/100 Iteration 1550 | Training loss: 4.1934\n",
      "Epoch 45/100 Iteration 1560 | Training loss: 4.0004\n",
      "Epoch 45/100 Iteration 1570 | Training loss: 4.0308\n",
      "Epoch 46/100 Iteration 1580 | Training loss: 4.0216\n",
      "Epoch 46/100 Iteration 1590 | Training loss: 4.0990\n",
      "Epoch 46/100 Iteration 1600 | Training loss: 4.2430\n",
      "Epoch 46/100 Iteration 1610 | Training loss: 4.1157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100 Iteration 1620 | Training loss: 4.1382\n",
      "Epoch 47/100 Iteration 1630 | Training loss: 3.9455\n",
      "Epoch 47/100 Iteration 1640 | Training loss: 4.0032\n",
      "Epoch 48/100 Iteration 1650 | Training loss: 3.9959\n",
      "Epoch 48/100 Iteration 1660 | Training loss: 4.0660\n",
      "Epoch 48/100 Iteration 1670 | Training loss: 4.2090\n",
      "Epoch 48/100 Iteration 1680 | Training loss: 4.0989\n",
      "Epoch 49/100 Iteration 1690 | Training loss: 4.1197\n",
      "Epoch 49/100 Iteration 1700 | Training loss: 3.9182\n",
      "Epoch 49/100 Iteration 1710 | Training loss: 3.9571\n",
      "Epoch 50/100 Iteration 1720 | Training loss: 3.9761\n",
      "Epoch 50/100 Iteration 1730 | Training loss: 4.0215\n",
      "Epoch 50/100 Iteration 1740 | Training loss: 4.1988\n",
      "Epoch 50/100 Iteration 1750 | Training loss: 4.0647\n",
      "Epoch 51/100 Iteration 1760 | Training loss: 4.1287\n",
      "Epoch 51/100 Iteration 1770 | Training loss: 3.9117\n",
      "Epoch 51/100 Iteration 1780 | Training loss: 3.9322\n",
      "Epoch 52/100 Iteration 1790 | Training loss: 3.9723\n",
      "Epoch 52/100 Iteration 1800 | Training loss: 4.0206\n",
      "Epoch 52/100 Iteration 1810 | Training loss: 4.1702\n",
      "Epoch 52/100 Iteration 1820 | Training loss: 4.0143\n",
      "Epoch 53/100 Iteration 1830 | Training loss: 4.0647\n",
      "Epoch 53/100 Iteration 1840 | Training loss: 3.8697\n",
      "Epoch 53/100 Iteration 1850 | Training loss: 3.9210\n",
      "Epoch 54/100 Iteration 1860 | Training loss: 3.9256\n",
      "Epoch 54/100 Iteration 1870 | Training loss: 3.9796\n",
      "Epoch 54/100 Iteration 1880 | Training loss: 4.1550\n",
      "Epoch 54/100 Iteration 1890 | Training loss: 4.0317\n",
      "Epoch 55/100 Iteration 1900 | Training loss: 4.0562\n",
      "Epoch 55/100 Iteration 1910 | Training loss: 3.8799\n",
      "Epoch 55/100 Iteration 1920 | Training loss: 3.8615\n",
      "Epoch 56/100 Iteration 1930 | Training loss: 3.8781\n",
      "Epoch 56/100 Iteration 1940 | Training loss: 3.9369\n",
      "Epoch 56/100 Iteration 1950 | Training loss: 4.1189\n",
      "Epoch 56/100 Iteration 1960 | Training loss: 3.9936\n",
      "Epoch 57/100 Iteration 1970 | Training loss: 4.0004\n",
      "Epoch 57/100 Iteration 1980 | Training loss: 3.8419\n",
      "Epoch 57/100 Iteration 1990 | Training loss: 3.8490\n",
      "Epoch 58/100 Iteration 2000 | Training loss: 3.8360\n",
      "Epoch 58/100 Iteration 2010 | Training loss: 3.9044\n",
      "Epoch 58/100 Iteration 2020 | Training loss: 4.0530\n",
      "Epoch 58/100 Iteration 2030 | Training loss: 3.9422\n",
      "Epoch 59/100 Iteration 2040 | Training loss: 3.9756\n",
      "Epoch 59/100 Iteration 2050 | Training loss: 3.7725\n",
      "Epoch 59/100 Iteration 2060 | Training loss: 3.7985\n",
      "Epoch 60/100 Iteration 2070 | Training loss: 3.8411\n",
      "Epoch 60/100 Iteration 2080 | Training loss: 3.8944\n",
      "Epoch 60/100 Iteration 2090 | Training loss: 4.0373\n",
      "Epoch 60/100 Iteration 2100 | Training loss: 3.9164\n",
      "Epoch 61/100 Iteration 2110 | Training loss: 3.9210\n",
      "Epoch 61/100 Iteration 2120 | Training loss: 3.7443\n",
      "Epoch 61/100 Iteration 2130 | Training loss: 3.8076\n",
      "Epoch 62/100 Iteration 2140 | Training loss: 3.8066\n",
      "Epoch 62/100 Iteration 2150 | Training loss: 3.8602\n",
      "Epoch 62/100 Iteration 2160 | Training loss: 3.9813\n",
      "Epoch 62/100 Iteration 2170 | Training loss: 3.8953\n",
      "Epoch 63/100 Iteration 2180 | Training loss: 3.9155\n",
      "Epoch 63/100 Iteration 2190 | Training loss: 3.7472\n",
      "Epoch 63/100 Iteration 2200 | Training loss: 3.7564\n",
      "Epoch 64/100 Iteration 2210 | Training loss: 3.8000\n",
      "Epoch 64/100 Iteration 2220 | Training loss: 3.8348\n",
      "Epoch 64/100 Iteration 2230 | Training loss: 3.9967\n",
      "Epoch 64/100 Iteration 2240 | Training loss: 3.8498\n",
      "Epoch 65/100 Iteration 2250 | Training loss: 3.8847\n",
      "Epoch 65/100 Iteration 2260 | Training loss: 3.7337\n",
      "Epoch 65/100 Iteration 2270 | Training loss: 3.7468\n",
      "Epoch 66/100 Iteration 2280 | Training loss: 3.7482\n",
      "Epoch 66/100 Iteration 2290 | Training loss: 3.7968\n",
      "Epoch 66/100 Iteration 2300 | Training loss: 3.9247\n",
      "Epoch 66/100 Iteration 2310 | Training loss: 3.8475\n",
      "Epoch 67/100 Iteration 2320 | Training loss: 3.9036\n",
      "Epoch 67/100 Iteration 2330 | Training loss: 3.7142\n",
      "Epoch 67/100 Iteration 2340 | Training loss: 3.7204\n",
      "Epoch 68/100 Iteration 2350 | Training loss: 3.7259\n",
      "Epoch 68/100 Iteration 2360 | Training loss: 3.8067\n",
      "Epoch 68/100 Iteration 2370 | Training loss: 3.9517\n",
      "Epoch 68/100 Iteration 2380 | Training loss: 3.8154\n",
      "Epoch 69/100 Iteration 2390 | Training loss: 3.8281\n",
      "Epoch 69/100 Iteration 2400 | Training loss: 3.6849\n",
      "Epoch 69/100 Iteration 2410 | Training loss: 3.6966\n",
      "Epoch 70/100 Iteration 2420 | Training loss: 3.7163\n",
      "Epoch 70/100 Iteration 2430 | Training loss: 3.7591\n",
      "Epoch 70/100 Iteration 2440 | Training loss: 3.9489\n",
      "Epoch 70/100 Iteration 2450 | Training loss: 3.8065\n",
      "Epoch 71/100 Iteration 2460 | Training loss: 3.8243\n",
      "Epoch 71/100 Iteration 2470 | Training loss: 3.6623\n",
      "Epoch 71/100 Iteration 2480 | Training loss: 3.6790\n",
      "Epoch 72/100 Iteration 2490 | Training loss: 3.7150\n",
      "Epoch 72/100 Iteration 2500 | Training loss: 3.7761\n",
      "Epoch 72/100 Iteration 2510 | Training loss: 3.9227\n",
      "Epoch 72/100 Iteration 2520 | Training loss: 3.8337\n",
      "Epoch 73/100 Iteration 2530 | Training loss: 3.8585\n",
      "Epoch 73/100 Iteration 2540 | Training loss: 3.6993\n",
      "Epoch 73/100 Iteration 2550 | Training loss: 3.6869\n",
      "Epoch 74/100 Iteration 2560 | Training loss: 3.6731\n",
      "Epoch 74/100 Iteration 2570 | Training loss: 3.7218\n",
      "Epoch 74/100 Iteration 2580 | Training loss: 3.8654\n",
      "Epoch 74/100 Iteration 2590 | Training loss: 3.7637\n",
      "Epoch 75/100 Iteration 2600 | Training loss: 3.7773\n",
      "Epoch 75/100 Iteration 2610 | Training loss: 3.6202\n",
      "Epoch 75/100 Iteration 2620 | Training loss: 3.6227\n",
      "Epoch 76/100 Iteration 2630 | Training loss: 3.6219\n",
      "Epoch 76/100 Iteration 2640 | Training loss: 3.7260\n",
      "Epoch 76/100 Iteration 2650 | Training loss: 3.8637\n",
      "Epoch 76/100 Iteration 2660 | Training loss: 3.7266\n",
      "Epoch 77/100 Iteration 2670 | Training loss: 3.7771\n",
      "Epoch 77/100 Iteration 2680 | Training loss: 3.6164\n",
      "Epoch 77/100 Iteration 2690 | Training loss: 3.5924\n",
      "Epoch 78/100 Iteration 2700 | Training loss: 3.5911\n",
      "Epoch 78/100 Iteration 2710 | Training loss: 3.6633\n",
      "Epoch 78/100 Iteration 2720 | Training loss: 3.8384\n",
      "Epoch 78/100 Iteration 2730 | Training loss: 3.7424\n",
      "Epoch 79/100 Iteration 2740 | Training loss: 3.7121\n",
      "Epoch 79/100 Iteration 2750 | Training loss: 3.5420\n",
      "Epoch 79/100 Iteration 2760 | Training loss: 3.5681\n",
      "Epoch 80/100 Iteration 2770 | Training loss: 3.5696\n",
      "Epoch 80/100 Iteration 2780 | Training loss: 3.6289\n",
      "Epoch 80/100 Iteration 2790 | Training loss: 3.7922\n",
      "Epoch 80/100 Iteration 2800 | Training loss: 3.7010\n",
      "Epoch 81/100 Iteration 2810 | Training loss: 3.7061\n",
      "Epoch 81/100 Iteration 2820 | Training loss: 3.5396\n",
      "Epoch 81/100 Iteration 2830 | Training loss: 3.5466\n",
      "Epoch 82/100 Iteration 2840 | Training loss: 3.5549\n",
      "Epoch 82/100 Iteration 2850 | Training loss: 3.6641\n",
      "Epoch 82/100 Iteration 2860 | Training loss: 3.7601\n",
      "Epoch 82/100 Iteration 2870 | Training loss: 3.6708\n",
      "Epoch 83/100 Iteration 2880 | Training loss: 3.6711\n",
      "Epoch 83/100 Iteration 2890 | Training loss: 3.5291\n",
      "Epoch 83/100 Iteration 2900 | Training loss: 3.5376\n",
      "Epoch 84/100 Iteration 2910 | Training loss: 3.5154\n",
      "Epoch 84/100 Iteration 2920 | Training loss: 3.6111\n",
      "Epoch 84/100 Iteration 2930 | Training loss: 3.8120\n",
      "Epoch 84/100 Iteration 2940 | Training loss: 3.6960\n",
      "Epoch 85/100 Iteration 2950 | Training loss: 3.6770\n",
      "Epoch 85/100 Iteration 2960 | Training loss: 3.5474\n",
      "Epoch 85/100 Iteration 2970 | Training loss: 3.5341\n",
      "Epoch 86/100 Iteration 2980 | Training loss: 3.4950\n",
      "Epoch 86/100 Iteration 2990 | Training loss: 3.6142\n",
      "Epoch 86/100 Iteration 3000 | Training loss: 3.7663\n",
      "Epoch 86/100 Iteration 3010 | Training loss: 3.6632\n",
      "Epoch 87/100 Iteration 3020 | Training loss: 3.6539\n",
      "Epoch 87/100 Iteration 3030 | Training loss: 3.5317\n",
      "Epoch 87/100 Iteration 3040 | Training loss: 3.5188\n",
      "Epoch 88/100 Iteration 3050 | Training loss: 3.4956\n",
      "Epoch 88/100 Iteration 3060 | Training loss: 3.6071\n",
      "Epoch 88/100 Iteration 3070 | Training loss: 3.7627\n",
      "Epoch 88/100 Iteration 3080 | Training loss: 3.6266\n",
      "Epoch 89/100 Iteration 3090 | Training loss: 3.6225\n",
      "Epoch 89/100 Iteration 3100 | Training loss: 3.4942\n",
      "Epoch 89/100 Iteration 3110 | Training loss: 3.4780\n",
      "Epoch 90/100 Iteration 3120 | Training loss: 3.4695\n",
      "Epoch 90/100 Iteration 3130 | Training loss: 3.5957\n",
      "Epoch 90/100 Iteration 3140 | Training loss: 3.7058\n",
      "Epoch 90/100 Iteration 3150 | Training loss: 3.6684\n",
      "Epoch 91/100 Iteration 3160 | Training loss: 3.6394\n",
      "Epoch 91/100 Iteration 3170 | Training loss: 3.4905\n",
      "Epoch 91/100 Iteration 3180 | Training loss: 3.4952\n",
      "Epoch 92/100 Iteration 3190 | Training loss: 3.4916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/100 Iteration 3200 | Training loss: 3.5422\n",
      "Epoch 92/100 Iteration 3210 | Training loss: 3.7073\n",
      "Epoch 92/100 Iteration 3220 | Training loss: 3.6163\n",
      "Epoch 93/100 Iteration 3230 | Training loss: 3.5711\n",
      "Epoch 93/100 Iteration 3240 | Training loss: 3.4284\n",
      "Epoch 93/100 Iteration 3250 | Training loss: 3.4553\n",
      "Epoch 94/100 Iteration 3260 | Training loss: 3.4473\n",
      "Epoch 94/100 Iteration 3270 | Training loss: 3.5324\n",
      "Epoch 94/100 Iteration 3280 | Training loss: 3.6558\n",
      "Epoch 94/100 Iteration 3290 | Training loss: 3.5949\n",
      "Epoch 95/100 Iteration 3300 | Training loss: 3.5468\n",
      "Epoch 95/100 Iteration 3310 | Training loss: 3.3980\n",
      "Epoch 95/100 Iteration 3320 | Training loss: 3.4141\n",
      "Epoch 96/100 Iteration 3330 | Training loss: 3.4202\n",
      "Epoch 96/100 Iteration 3340 | Training loss: 3.4747\n",
      "Epoch 96/100 Iteration 3350 | Training loss: 3.6403\n",
      "Epoch 96/100 Iteration 3360 | Training loss: 3.5399\n",
      "Epoch 97/100 Iteration 3370 | Training loss: 3.5310\n",
      "Epoch 97/100 Iteration 3380 | Training loss: 3.4216\n",
      "Epoch 97/100 Iteration 3390 | Training loss: 3.3886\n",
      "Epoch 98/100 Iteration 3400 | Training loss: 3.4105\n",
      "Epoch 98/100 Iteration 3410 | Training loss: 3.4958\n",
      "Epoch 98/100 Iteration 3420 | Training loss: 3.6175\n",
      "Epoch 98/100 Iteration 3430 | Training loss: 3.5376\n",
      "Epoch 99/100 Iteration 3440 | Training loss: 3.5308\n",
      "Epoch 99/100 Iteration 3450 | Training loss: 3.3707\n",
      "Epoch 99/100 Iteration 3460 | Training loss: 3.4069\n",
      "Epoch 100/100 Iteration 3470 | Training loss: 3.3953\n",
      "Epoch 100/100 Iteration 3480 | Training loss: 3.4479\n",
      "Epoch 100/100 Iteration 3490 | Training loss: 3.6115\n",
      "Epoch 100/100 Iteration 3500 | Training loss: 3.5311\n",
      "CPU times: user 4h 50min 45s, sys: 28min 47s, total: 5h 19min 33s\n",
      "Wall time: 46min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_epochs=100\n",
    "ckpt_dir = './model-{}/'.format(num_epochs)\n",
    "lstm.train(x_train, y_train, num_epochs=num_epochs, ckpt_dir=ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_outputs: Tensor(\"rnn/transpose_1:0\", shape=(1, 1, 256), dtype=float32)\n",
      "Tensor(\"probabilities:0\", shape=(1, 3046), dtype=float32)\n",
      "INFO:tensorflow:Restoring parameters from ./model-100/lyrics_generator.ckpt\n"
     ]
    }
   ],
   "source": [
    "## rebuild lstm with input tensor shape (1,1) for sampling/generating\n",
    "try: del lstm\n",
    "except NameError: pass\n",
    "\n",
    "np.random.seed(np.random.randint(200))\n",
    "lstm = LyricsGeneratorNN(token2int, int2token, embedding_matrix, mini_seq_len=mini_seq_len,\n",
    "                         batch_size=batch_size, num_nodes=num_nodes, num_layers=num_layers, sampling=True)\n",
    "generated_text = lstm.sample(ckpt_dir='./model-100/', output_length=400, starter_tokens=[\"You\", \"got\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for char in punctuation:\n",
    "    if punctuation not in [\"'\",]:\n",
    "        generated_text = generated_text.replace(' '+char, char)\n",
    "\n",
    "# maybe move this to preprocessing step\n",
    "for i in range(0, 2):\n",
    "    generated_text = generated_text.replace('\\n \\n', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You got to do you love to \n",
      " You have got the world to see the way \n",
      " I don't want to be my mind \n",
      " I will have to get my heart \n",
      " You know I don't know why I have not seen to take \n",
      " I got no little man to love \n",
      " \n",
      " \n",
      " You don't know how much you \n",
      " I know I can be in the \n",
      " In the \n",
      " I can get my mind \n",
      " And I am so much of the \n",
      " You know you can do my heart \n",
      " I don't know why \n",
      " And I can't see my name \n",
      " And I know you're going to be raining \n",
      " I know that I could get in a sky \n",
      " You know that I do not know \n",
      " I got no woman in my eyes on your mind \n",
      " And I don't know where I am a blues \n",
      " I don't have to be right in clay \n",
      " I got a lot of day \n",
      " I have finally no friend \n",
      " \n",
      " You got the woman of my head \n",
      " \n",
      " You can be in my eyes \n",
      " And I'm gonna be right \n",
      " You can be in your mind \n",
      " \n",
      " And my love is dead, \n",
      " I don't know why, \n",
      " \n",
      " \n",
      " \n",
      " I don't care in the \n",
      " And I'm going to be so lonesome \n",
      " I don't know why, and I do not know how love to do \n",
      " I don't lie \n",
      " I don't care why If I know I do not see you \n",
      " I know that I'm going to be raining in my mind \n",
      " I got to get the way to get your heart \n",
      " You got a whole like the world \n",
      " \n",
      " And I'm not going to be like the time \n",
      " I have got a one of day \n",
      " I got a blues of a \n",
      " I know I don't know how \n",
      " I have been so sad? \n",
      " \n",
      " I know I don't\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36_dev",
   "language": "python",
   "name": "py36_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
