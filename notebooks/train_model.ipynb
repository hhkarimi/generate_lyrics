{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## based on:\n",
    "## https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/code/ch16/ch16.ipynb\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load embedding matrix and vocabulary\n",
    "path = './data/'\n",
    "vocab = np.load(path + 'vocab.npy')\n",
    "embedding_matrix = np.load(path + 'embedding_matrix.npy') # shape = [vocab_size, embedding_size]\n",
    "\n",
    "# create dictionary{word: int}\n",
    "token2int = {token: i for i, token in enumerate(vocab, 1)}\n",
    "# for i, token in enumerate(vocab):\n",
    "#     token2int[token] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eagles</td>\n",
       "      <td>After The Thrill Is Gone</td>\n",
       "      <td>[[, Verse, ], ., Same, dances, in, the, same, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eagles</td>\n",
       "      <td>All She Wants To Do Is Dance</td>\n",
       "      <td>[They, 're, pickin, ', up, the, prisoners, ., ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eagles</td>\n",
       "      <td>Already Gone</td>\n",
       "      <td>[[, Verse, 1, ], ., Well, ,, I, heard, some, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eagles</td>\n",
       "      <td>Best of My Love</td>\n",
       "      <td>[[, Verse, 1, ], ., Every, night, I, 'm, lying...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eagles</td>\n",
       "      <td>Bitter Creek</td>\n",
       "      <td>[Once, I, was, young, and, so, unsure, ., I, '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   artist                          song  \\\n",
       "0  Eagles      After The Thrill Is Gone   \n",
       "1  Eagles  All She Wants To Do Is Dance   \n",
       "2  Eagles                  Already Gone   \n",
       "3  Eagles               Best of My Love   \n",
       "4  Eagles                  Bitter Creek   \n",
       "\n",
       "                                              lyrics  \n",
       "0  [[, Verse, ], ., Same, dances, in, the, same, ...  \n",
       "1  [They, 're, pickin, ', up, the, prisoners, ., ...  \n",
       "2  [[, Verse, 1, ], ., Well, ,, I, heard, some, p...  \n",
       "3  [[, Verse, 1, ], ., Every, night, I, 'm, lying...  \n",
       "4  [Once, I, was, young, and, so, unsure, ., I, '...  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load and tokenize lyrics\n",
    "with open('./data/lyrics_by_artist/Eagles.json') as f:\n",
    "    tmp = json.load(f)\n",
    "data = pd.DataFrame(columns=['artist', 'song', 'lyrics'])\n",
    "for i in range(0, len(tmp['artists'][0]['songs'])):\n",
    "    data = data.append({'artist': tmp['artists'][0]['artist'],\n",
    "                        'song': tmp['artists'][0]['songs'][i]['title'],\n",
    "                         'lyrics': tmp['artists'][0]['songs'][i]['lyrics']},\n",
    "                         ignore_index=True)\n",
    "    \n",
    "data['lyrics'] = data['lyrics'].apply(lambda text: word_tokenize(text.replace('\\n', '. ')))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.5 ms, sys: 1.18 ms, total: 11.7 ms\n",
      "Wall time: 10.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# convert words/tokens to integers from dictionary\n",
    "def tokens_to_int(tokens):\n",
    "    mapped_tokens = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            mapped_tokens.append(token2int[token])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return mapped_tokens\n",
    "    \n",
    "data['lyrics'] = data['lyrics'].apply(tokens_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean num_tokens: 275.6 \n",
      "min num_tokens: 1 \n",
      "max num_tokens: 539\n"
     ]
    }
   ],
   "source": [
    "# pad/clip each set song based on clip/pad based on mean/min/max song length\n",
    "print(\n",
    "    'mean num_tokens: {:.1f}'.format(data['lyrics'].apply(lambda x: len(x)).mean()),\n",
    "    '\\nmin num_tokens: {:d}'.format(data['lyrics'].apply(lambda x: len(x)).min()),\n",
    "    '\\nmax num_tokens: {:d}'.format(data['lyrics'].apply(lambda x: len(x)).max())\n",
    "    )\n",
    "\n",
    "mean_lyrics_length = data['lyrics'].apply(lambda x: len(x)).mean()\n",
    "max_lyrics_length = data['lyrics'].apply(lambda x: len(x)).max()\n",
    "sequence_length = int(np.mean([mean_lyrics_length, max_lyrics_length]))\n",
    "sequences = np.zeros((len(data), sequence_length), dtype=int)\n",
    "for i, row in enumerate(data['lyrics'].values):\n",
    "    sequences[i, -len(row):] = row[-sequence_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set target to predict the next word in the sequence, so y is x offset by 1 position\n",
    "train_percentage = 0.8\n",
    "train_length = int(train_percentage*len(data))\n",
    "\n",
    "X_train = sequences[:train_length, :]\n",
    "y_train = np.zeros(X_train.shape, dtype=int)\n",
    "y_train[:, :-1] = X_train[:, 1:]\n",
    "\n",
    "X_test = sequences[train_length:, :]\n",
    "y_test = np.zeros(X_test.shape, dtype=int)\n",
    "y_test[:, :-1] = X_test[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(21)\n",
    "\n",
    "def create_batch_generator(x, y=None, batch_size=8):\n",
    "    n_batches = len(x)//batch_size # // is floor division\n",
    "    x = x[:n_batches*batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build LSTM model\n",
    "\n",
    "class LyricsGenerator(object):\n",
    "    def __init__(self, token2int, embedding_matrix, batch_size=64, seq_len=100, num_nodes=128,\n",
    "                 num_layers=1, learning_rate=0.001, keep_prob=0.5, grad_clip=5, \n",
    "                 sampling=False):\n",
    "        self.num_tokens = len(token2int)\n",
    "        self.token2int = token2int\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_layers = num_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.keep_prob = keep_prob\n",
    "        self.grad_clip = grad_clip\n",
    "        \n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(21)\n",
    "            \n",
    "            self.build(sampling=sampling)\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            \n",
    "    def build(self, sampling):\n",
    "        if sampling == True:\n",
    "            batch_size, seq_len = 1, 1\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "            seq_len = self.seq_len\n",
    "            \n",
    "        tf_x = tf.placeholder(tf.int32, shape=[batch_size, seq_len], name='tf_x')\n",
    "        tf_y = tf.placeholder(tf.int32, shape=[batch_size, seq_len], name='tf_y')\n",
    "        tf_keep_prob = tf.placeholder(tf.float32, name='tf_keep_prob')\n",
    "        \n",
    "        # load the embedding layer\n",
    "        embedding = tf.constant(self.embedding_matrix, name='embedding')\n",
    "        embed_x = tf.nn.embedding_lookup(embedding, tf_x, name='embedded_x')\n",
    "        \n",
    "        # one-hot encoding\n",
    "        x_onehot = tf.one_hot(tf_x, depth=self.num_tokens)\n",
    "        y_onehot = tf.one_hot(tf_y, depth=self.num_tokens)\n",
    "        \n",
    "        # build the multi-layer LSTM cells\n",
    "        cells = tf.contrib.rnn.MultiRNNCell(\n",
    "            [tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(self.num_nodes), output_keep_prob=tf_keep_prob)\n",
    "            for _ in range(0, self.num_layers)])\n",
    "        \n",
    "        # set initial state\n",
    "        self.initial_state = cells.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        # run each sequence step through the RNN\n",
    "#         lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n",
    "#             cells, x_onehot, initial_state=self.initial_state)\n",
    "\n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n",
    "            cells, embed_x, initial_state=self.initial_state)\n",
    "        print('lstm_outputs:', lstm_outputs)\n",
    "        \n",
    "        seq_output_reshaped = tf.reshape(lstm_outputs, shape=[-1, self.num_nodes],\n",
    "                                         name='seq_output_reshaped')\n",
    "        \n",
    "        logits = tf.layers.dense(inputs=seq_output_reshaped, units=self.num_tokens,\n",
    "                                 activation=None, name='logits')\n",
    "        \n",
    "        proba = tf.nn.softmax(logits, name='probabilities')\n",
    "        print(proba)\n",
    "        \n",
    "        y_reshaped = tf.reshape(y_onehot, shape=[-1, self.num_tokens], name='y_reshaped')\n",
    "#         y_reshaped = tf.reshape(tf_y, shape=[-1, self.num_tokens], name='y_reshaped')\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            logits=logits, labels=y_reshaped), name='cost')\n",
    "        \n",
    "        # gradient clipping to avoid exploding gradients\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), self.grad_clip)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars), name='train_op')\n",
    "        \n",
    "    def train(self, train_x, train_y, num_epochs, ckpt_dir='./model/'):\n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.mkdir(ckpt_dir)\n",
    "            \n",
    "        with tf.Session(graph=self.g) as session:\n",
    "            session.run(self.init_op)\n",
    "            \n",
    "            n_batches = int(train_x.shape[1]/self.seq_len) # check this arithmetic\n",
    "            iterations = n_batches * num_epochs\n",
    "            for epoch in range(0, num_epochs):\n",
    "                # train network\n",
    "                new_state = session.run(self.initial_state)\n",
    "                loss = 0\n",
    "                # minibatch operator\n",
    "                bgen = create_batch_generator(train_x, train_y, self.seq_len)\n",
    "                for b, (batch_x, batch_y) in enumerate(bgen, 1):\n",
    "                    iteration = epoch*n_batches + b\n",
    "                    \n",
    "                    feed = {'tf_x:0': batch_x, 'tf_y:0': batch_y, \n",
    "                            'tf_keep_prob:0': self.keep_prob, self.initial_state: new_state}\n",
    "                    batch_cost, _, new_state = session.run(\n",
    "                        ['cost:0', 'train_op', self.final_state], feed_dict=feed)\n",
    "                    \n",
    "                    if iteration % 10 == 0:\n",
    "                        print('Epoch {:d}/{:d} Iteration {:d} | Training loss: {:.4f}'.format(\n",
    "                            epoch+1, num_epochs, iteration, batch_cost))\n",
    "                        \n",
    "                ## save trained model\n",
    "                self.saver.save(session, os.path.join(\n",
    "                    ckpt_dir, 'lyrics_generator.ckpt'))\n",
    "                \n",
    "    def sample(self, output_length, ckpt_dir, starter_tokens=[\"The\", \"rain\"]):\n",
    "        with tf.Session(graph=self.g) as session:\n",
    "            self.saver.restore(session, tf.train.latest_checkpoint(ckpt_dir))\n",
    "            \n",
    "            # 1: run the model using starter tokens\n",
    "            new_state = sess.run(self.initial_state)\n",
    "            for token in starter_tokens:\n",
    "                x = np.zeros((1,1))\n",
    "                x[0, 0] = dictionary[token]\n",
    "                \n",
    "                feed = {'tf_x:0': x, 'tf_keep_prob:0': 1.0, self.initial_state: new_state}\n",
    "                proba, new_state = session.run(\n",
    "                    ['probabilities:0', self.final_state], feed_dict=feed)\n",
    "                \n",
    "            token_id = self.get_top_token(proba, len(vocab))\n",
    "            observed_seq.append(vocab[token_id])\n",
    "                \n",
    "            # 2: run model using updated observed_seq\n",
    "            for i in range(0, output_length):\n",
    "                x[0, 0] = token_id\n",
    "                feed = {'tf_x:0': x, 'tf_keep_prob:0': 1.0,\n",
    "                        self.initial_state: new_state}\n",
    "                proba, new_state = session.run(\n",
    "                    ['probabilities:0', self.final_state], feed_dict=feed)\n",
    "                \n",
    "                token_id = self.get_top_token(proba, len(vocab))\n",
    "                observed_seq.append(vocab[token_id])\n",
    "                \n",
    "            return ''.join(observed_seq)\n",
    "        \n",
    "    def get_top_token(self, probas, token_size, top_n=5):\n",
    "        p = np.squeeze(probas)\n",
    "        p[np.argsort(p)[:-top_n]] = 0.0\n",
    "        p = p / np.sum(p)\n",
    "        token_id = np.random.choice(char_size, 1, p=p)[0]\n",
    "        return token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_outputs: Tensor(\"rnn/transpose_1:0\", shape=(8, 100, 128), dtype=float32)\n",
      "Tensor(\"probabilities:0\", shape=(800, 24603), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "lstm = LyricsGenerator(token2int, embedding_matrix, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36_dev",
   "language": "python",
   "name": "py36_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
